{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "In this exercise sheet we look into how to compute and apply a Principal Component Analysis (PCA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy 4D Example\n",
    "\n",
    "We start by loading our toy example. The data is stored as a Numpy array, it is a $2585\\times 5$ matrix. The last component of each row is the label, the first four components are the coordinates in 4D. Each label is an integer from  $\\{0, 1, 2, 3, 4\\}$.\n",
    "\n",
    "The data contains a noisy 2D plane which is embded into 4D. We would like to represent the data in its _intrinsic_ 2D form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pillow  # install the Python package \"pillow\"\n",
    "import numpy as np\n",
    "import mllab.pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_toy_4d = np.load(\"data/pca_toy_4d.npy\")\n",
    "y = pca_toy_4d[:, -1]  # labels\n",
    "x = pca_toy_4d[:, :-1]  # 4D coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot slices from this 4D data. We provide a helper function for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show documentation\n",
    "mllab.pca.plot_toy_slice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mllab.pca.plot_toy_slice(x, y, drop_dim=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to remove the noise and recover the 2D information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1\n",
    "\n",
    "Write an implementation of the function below. Use a singular value decomposition (SVD), but avoid computing it completely since we only need the first $q$ eigenvectors. You can use a NumPy/SciPy function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "def pca(x, q):\n",
    "    \"\"\"\n",
    "    Compute principal components and the coordinates.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    x: (n, d) NumPy array\n",
    "    q: int\n",
    "       The number of principal components to compute.\n",
    "       Has to be less than `p`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    Vq: (d, q) NumPy array, orthonormal vectors (column-wise)\n",
    "    xq: (n, q) NumPy array, coordinates for x (row-wise)\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the 2D dimensional representation of `x` using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V, xq = pca(x, q=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then plot the coordinates `xq`, which are two dimensional. We provide a helper function for this task. Let us check how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mllab.pca.plot_toy_2d?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xq = mllab.pca.plot_toy_2d(xq, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully you appreciate the result.\n",
    "\n",
    "### Task 3.2\n",
    "\n",
    "Let us see how PCA handles a non-linear transformation. To test this we map our data into 3D by keeping the y-axis as the new z-axis and bending x-coordinate onto an ellipse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz = mllab.pca.map_on_ellipse(xq, a=32, b=1, gap_angle=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "mllab.pca.plot_toy_3d(xyz, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Remeber to stop the interactive plot by pressing the shutdown icon!)**\n",
    "\n",
    "Now apply PCA to our transformed data and plot the result as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# plot code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could be worse, but undeniably discomforting. Try different axes lengths and gap sizes of the ellipse. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3\n",
    "\n",
    "We want to see if PCA can improve the accuracy of separating hyperlanes. First compute the singular values of the Iris dataset, then check how many percent of the variance the first two principal components capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "iris_x = iris['data']\n",
    "iris_y = iris['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import svdvals\n",
    "#Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply PCA and compute the first two principal components. Plot the projected 2D data in a scatter plot such that the three labels are recognizable. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_1d_iris(a, b, c):\n",
    "    \"\"\"Show a 1D plot of three 1D datasets a, b and c.\n",
    "    \n",
    "    Top to bottom plotted in order is a, b, c.\"\"\"\n",
    "    left = min(x.min() for x in (a, b, c))\n",
    "    right = max(x.max() for x in (a, b, c))\n",
    "    for i, (x, c) in enumerate(((a, 'red'), (b, 'blue'), (c, 'green'))):\n",
    "        plt.hlines(i * .3, left, right, linestyles='dotted', colors=[(.8,.8,.8,1)])\n",
    "        plt.eventplot(x, colors=c, linewidths=.5, linelengths=.25, lineoffsets=(2 - i) * .3)\n",
    "    plt.axis('off')\n",
    "\n",
    "# your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, recompute the accurancies and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import lstsq\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def labels(x1, x2):\n",
    "    return np.concatenate((np.zeros(x1.shape[0], dtype='int'), np.ones(x2.shape[0], dtype='int')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pedestrian Classification\n",
    "\n",
    "__Read the pedestrian dataset into a NumPy array and normalize to [0,1]__ (Task 5.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mllab.pca\n",
    "mllab.pca.load_pedestrian_images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Write a function to plot an image__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "def plot_im(im, ax=None, title=None, max_contrast=False):\n",
    "    \"\"\"\n",
    "    Plot a normalized image.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    im: (1250,) array-like\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Plot 10 randomly chosen images showing a pedestrian, and 10 randomly chosen images not showing a pedestrain__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Compute the PCA of the full training set for $q=200$__ (Task 5.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Plot the eigenpedestrian 1-20, 51-60, and 101-110__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that higher eigenvectors (eigenpedestrians) correspond to higher frequencies in the image data. For the first eigenpedestrians we indeed recognize the shape of a human being, whereas the latter ones seem to have high frequency fluctuations/noise in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Compute the scores for a linear SVM using increasing numbers of principal components__ (Task 5.6)\n",
    "\n",
    "Use 10 to 200 components in steps of 5. Train the linear SVM with $C=0.01$ and increse the maximum number of iterations for the solver. You can reuse the computed PCA from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training and test scores over $q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that too small values of q lead to worse accuracies on both training and test data (underfitting), whereas high values of q lead to high training accuracies but low test accuracies (overfitting). The sweet spot seems to be somewhere in the middle (roughly q = 50)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HOG Features\n",
    "\n",
    "__Implementation of the HOG features__\n",
    "\n",
    "Finally, we want to see if we can increase the accuracies by using well-tailored features such as the HoG features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.ndimage as ndimage\n",
    "from numpy.linalg import norm\n",
    "from scipy.ndimage import convolve\n",
    "\n",
    "\n",
    "class HogFeatures:\n",
    "    def __init__(self, im_shape, n_bins=9, cell_size=8, blk_size=2, unsigned=True, clip_val=.2):\n",
    "        self.deg_range = np.pi if unsigned else 2 * np.pi\n",
    "        self.n_bins = n_bins\n",
    "        self.bins = np.linspace(0, self.deg_range, n_bins, endpoint=False)\n",
    "        self.bin_size = self.deg_range / n_bins\n",
    "        self.cell_size = cell_size\n",
    "        self.blk_size = blk_size\n",
    "        self.clip_val = clip_val\n",
    "\n",
    "\n",
    "        self.im_h, self.im_w = im_shape\n",
    "        x, y = np.arange(self.im_w), np.arange(self.im_h)\n",
    "        \n",
    "        # Compute logical cell indices of next lower and upper cell\n",
    "        # w.r.t. to the cell center\n",
    "        cells_x = np.arange(-cell_size, self.im_w - (cell_size + 1)/2, cell_size)\n",
    "        self.n_cells_x = len(cells_x) - (2 if cells_x[-1] >= self.im_w else 1)\n",
    "        x0 = np.digitize(x, cells_x + cell_size / 2) - 2\n",
    "        Xc = ((x0 + 1) - .5) * cell_size - .5\n",
    "        f_x = (x - Xc) / cell_size\n",
    "\n",
    "        cells_y = np.arange(-cell_size, self.im_h - (cell_size + 1)/2, cell_size)\n",
    "        self.n_cells_y = len(cells_y) - (2 if cells_y[-1] >= self.im_h else 1)\n",
    "        y0 = np.digitize(y, cells_y + cell_size / 2) - 2\n",
    "        Yc = ((y0 + 1) - .5) * cell_size - .5\n",
    "        f_y = (y - Yc) / cell_size\n",
    "        \n",
    "        self.f_x, self.f_y = np.meshgrid(f_x, f_y)\n",
    "\n",
    "    \n",
    "    def extract(self, im):\n",
    "        \"\"\"\n",
    "        Extract the HOG features for a image.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        im: ndarray\n",
    "            An array of shape (height, width, 3).\n",
    "        \"\"\"\n",
    "\n",
    "        im = np.rollaxis(im.reshape(self.im_h, self.im_w, -1), 2)\n",
    "        dx = convolve(im, [[[1,0,-1]]], mode='constant')\n",
    "        dy = convolve(im, [[[-1],[0],[1]]], mode='constant')\n",
    "        grads_mag = norm(np.stack((dx, dy), axis=-1), axis=3)\n",
    "        max_grads = np.argmax(np.rollaxis(grads_mag, 0, 3), 2)\n",
    "        Y, X = np.ogrid[:grads_mag.shape[1], :grads_mag.shape[2]]\n",
    "        grads_dir = np.arctan2(dy[max_grads, Y, X], dx[max_grads, Y, X]) % self.deg_range\n",
    "        grads_mag = grads_mag[max_grads, Y, X]\n",
    "        del dx, dy, max_grads, Y, X\n",
    "        \n",
    "        # Compute logical bin indices of next lower (<=) and upper bin (>)\n",
    "        # w.r.t. to the bin center\n",
    "        bin0 = np.digitize(grads_dir, self.bins + .5 * self.bin_size) - 1\n",
    "        bin1 = bin0 + 1\n",
    "        dirc = (bin0 + .5) * self.bin_size\n",
    "        f_b = (grads_dir - dirc) / self.bin_size\n",
    "        del grads_dir\n",
    "        \n",
    "        bin0 %= self.n_bins\n",
    "        bin1 %= self.n_bins\n",
    "        \n",
    "        f_x, f_y = self.f_x, self.f_y\n",
    "\n",
    "        hist = np.zeros((self.n_cells_y, self.n_cells_x, self.n_bins))\n",
    "        bin_labels = np.arange(self.n_bins)\n",
    "        # Iterate over all cells\n",
    "        for ci_x in range(self.n_cells_x):\n",
    "            x_pos = (ci_x * self.cell_size - (self.cell_size + 1) // 2, ci_x * self.cell_size + (self.cell_size + 1) // 2)\n",
    "            x_pre = slice(max(0, x_pos[0] + self.cell_size), max(0, x_pos[1] + self.cell_size))\n",
    "            x_pos = slice(max(0, x_pos[0]), x_pos[1])\n",
    "            for ci_y in range(self.n_cells_y):\n",
    "                y_pos = (ci_y * self.cell_size - (self.cell_size + 1) // 2, ci_y * self.cell_size + (self.cell_size + 1) // 2)\n",
    "                y_pre = slice(max(0, y_pos[0] + self.cell_size), max(0, y_pos[1] + self.cell_size))\n",
    "                y_pos = slice(max(0, y_pos[0]), y_pos[1])\n",
    "                # Consider all four sourinding cells\n",
    "                    \n",
    "                # y-pre x-pre\n",
    "                m = (y_pre, x_pre)\n",
    "                g = grads_mag[m] * (1 - f_x[m]) * (1 - f_y[m])\n",
    "                hist[ci_y, ci_x] += ndimage.sum(g * (1 - f_b[m]), bin0[m], bin_labels) + \\\n",
    "                    ndimage.sum(g * f_b[m], bin1[m], bin_labels)\n",
    "                # y-pos x-pre\n",
    "                m = (y_pos, x_pre)\n",
    "                g = grads_mag[m] * (1 - f_x[m]) * f_y[m]\n",
    "                hist[ci_y, ci_x] += ndimage.sum(g * (1 - f_b[m]), bin0[m], bin_labels) + \\\n",
    "                    ndimage.sum(g * f_b[m], bin1[m], bin_labels)\n",
    "                # y-pre x-pos\n",
    "                m = (y_pre, x_pos)\n",
    "                g = grads_mag[m] * f_x[m] * (1 - f_y[m])\n",
    "                hist[ci_y, ci_x] += ndimage.sum(g * (1 - f_b[m]), bin0[m], bin_labels) + \\\n",
    "                    ndimage.sum(g * f_b[m], bin1[m], bin_labels)\n",
    "                # y-pos x-pos\n",
    "                m = (y_pos, x_pos)\n",
    "                g = grads_mag[m] * f_x[m] * f_y[m]\n",
    "                hist[ci_y, ci_x] += ndimage.sum(g * (1 - f_b[m]), bin0[m], bin_labels) + \\\n",
    "                    ndimage.sum(g * f_b[m], bin1[m], bin_labels)\n",
    "        \n",
    "        n_blks_x = self.n_cells_x + 1 - self.blk_size\n",
    "        n_blks_y = self.n_cells_y + 1 - self.blk_size\n",
    "        features = np.zeros((n_blks_x, n_blks_y, self.blk_size ** 2 * self.n_bins))\n",
    "        for bi_x in range(n_blks_x):\n",
    "            for bi_y in range(n_blks_y):\n",
    "                blk = hist[bi_y:bi_y+self.blk_size, bi_x:bi_x+self.blk_size].copy()\n",
    "                blk_norm = norm(blk.flatten())\n",
    "                if blk_norm > 0:\n",
    "                    blk /= blk_norm\n",
    "                np.clip(blk, None, self.clip_val, out=blk)\n",
    "                blk_norm = norm(blk.flatten())\n",
    "                if blk_norm > 0:\n",
    "                    blk /= blk_norm\n",
    "                features[bi_x, bi_y] = blk.ravel()\n",
    "        return features.flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Compute the HOG features for the training data, then compute the PCA for $q=200$.__ (Task 5.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install tqdm\n",
    "hog = HogFeatures((100, 50))\n",
    "\n",
    "from tqdm import tqdm\n",
    "hog_train_features = []\n",
    "for i in tqdm(range(int_train_features.shape[0])):\n",
    "    im = int_train_features[i].reshape(100, 50, 3)\n",
    "    hog_train_features.append(hog.extract(im))\n",
    "print(\"Computed HoG.\")\n",
    "\n",
    "q = 200\n",
    "hog_train_features = np.array(hog_train_features)\n",
    "hog_train_pca = PCA(n_components=q)\n",
    "hog_train_pca.fit(hog_train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Compute and plot the scores as above, but this time use the HOG features.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute scores\n",
    "# your code here \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "# your code here\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
