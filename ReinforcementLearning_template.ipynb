{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4f0023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaaaf7d",
   "metadata": {},
   "source": [
    "## 6.1. Introduction to Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c3ce3e",
   "metadata": {},
   "source": [
    "For the problem formulation, we introduce the [gymnasium](https://gymnasium.farama.org/) library. It implements control problems from the past and present of reinforcement learning that have served as milestones in the development of that technique. Researchers that work on the same standard problems have the advantage that their work is easier to compare and to transfer. On the other hand, if benchmark problems are too prevalent in a community, it may drive research in a certain, uniform direction that is not as productive anymore. Note that gym is a product of OpenAI, a private company. \n",
    "\n",
    "gym uses a unifying framework that defines every control problem as an *environment*. The basic building blocks of an environment are `env = gym.make` to create the environment, `env.reset` to start an episode, `env.render` to give a human readable representation of the state of the environment, and `env.step` to perform an action.\n",
    "\n",
    "We start the exercises with the 4x4 [FrozenLake](https://gymnasium.farama.org/environments/toy_text/frozen_lake/) environment. It is a kind of maze with \"frozen\" traversable squares marked by `F` and \"holes\", losing terminal squares marked by `H`. The agent starts at the `S` start square and only incurs reward, when they manage to get to the goal `G` square. We mostly look at the deterministic case, where traversing on the frozen lake is deterministic, which is controlled by the variable `is_slippery=False` when creating the environment. If the lake is slippery, a movement in a certain direction may by chance result in the agent arriving at a different square than expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3ef494",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=\"human\")\n",
    "#print(env.action_space)\n",
    "#print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0af69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_state, _ = env.reset()\n",
    "#print(starting_state)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34850d84",
   "metadata": {},
   "source": [
    "The `env.action_space` always implements a `sample` method, which returns a valid, random aciton. We can utilize this, to have a look at the dynamics of the system. You can execute the following cell a few times to see what happens. When the agent enters a terminal state, you need to execute `env.reset` to start anew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47af7c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "print(state, reward, terminated, truncated, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdeb143e",
   "metadata": {},
   "source": [
    "#### Task 1. a) Random Agent:\n",
    "We provide the framework for the random agent, a method to rollout a policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb60f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(env, agent):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = agent.action(state)\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "    return total_reward\n",
    "\n",
    "class RandomAgent:\n",
    "    def __init__(self, action_space, observation_space):\n",
    "        self.action_space = action_space\n",
    "        self.observation_space = observation_space\n",
    "        \n",
    "    # We pass the state only for compatability\n",
    "    def action(self, state):\n",
    "    # your code goes here\n",
    "        return None\n",
    "    \n",
    "def compute_avg_return(env, agent, num_episodes=5000):\n",
    "    # your code goes here\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d85a08a",
   "metadata": {},
   "source": [
    "Add your code to estimate the `avg_return_random_agent` for the deterministic case and `avg_return_random_agent_slippery` for the stochastic case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1754c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=None)\n",
    "# your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec8e8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Estimation for the deterministic case:\", avg_return_random_agent)\n",
    "print(\"Estimation for the stochastic case:\", avg_return_random_agent_slippery)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05c63c3",
   "metadata": {},
   "source": [
    "### 1. b) Iterative Policy Evaluation\n",
    "We provide a `set_state` method that changes the state of the environment. This is a pretty unusual way to interact with this framework. Note, that the random policy is stochastic, while the environment is not. In the value update we sum the value of each possible action that is weighted by its probability to be picked by the action. The architecture of the agent does provide access to these inner dynamics, so instead of passing the agent or its dynamics as a variable, we implement iterative policy evaluation just for the random agent, with the probability of `0.25` for each action hard coded.\n",
    "\n",
    "We also provide `all_states` and `all_actions`, lists of all admissable states and actions for the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce56d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states = list(range(env.observation_space.n))\n",
    "all_actions = list(range(env.action_space.n))\n",
    "\n",
    "def set_state(env, state):\n",
    "    env.reset()\n",
    "    env.env.env.env.s = state\n",
    "    return env\n",
    "\n",
    "def visualize_value_fct(v):\n",
    "    print(np.round(np.array(list(v.values())).reshape((4,4)),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a32c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_policy_iteration_random_agent(env, all_states, all_actions, discount_rate, \n",
    "                                            threshold=0.001, max_iter=10000):\n",
    "    v = {s: 0 for s in all_states}  # value function, initialized to 0\n",
    "    # your code goes here\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35557fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_random = iterative_policy_iteration_random_agent(env, all_states, all_actions, discount_rate=0.9)\n",
    "visualize_value_fct(v_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00941b0",
   "metadata": {},
   "source": [
    "### 1. c) Value Iteration\n",
    "Use value iteration to find the optimal policy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61587995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, all_states, all_actions, discount_rate, threshold=0.001, max_iter=10000):\n",
    "    v = {s: 0 for s in all_states}  # value function, initialized to 0\n",
    "    # your code goes here\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d68d13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_optimal = value_iteration(env, all_states, all_actions, discount_rate=0.9)\n",
    "visualize_value_fct(v_optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd5617c",
   "metadata": {},
   "source": [
    "### 2. a) Sarsa & Q-Learning\n",
    "With the language of a Q-table, we can define a more general agent by a Q-function.\n",
    "\n",
    "*Please do not use* `set_state` *anymore! Instead always start an episode with* `state = env.reset()`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7e6b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_q_fct(q):\n",
    "    acts = {0 : \"L\", 1 : \"D\", 2 : \"R\", 3 : \"U\"} \n",
    "    for j in range(4):\n",
    "        print(\"Value for action\", acts[j], \":\")\n",
    "        print(np.round(np.array([q[i][j] for i in range(16)]).reshape((4,4)), 3))\n",
    "    for i in range(4):\n",
    "        print([acts[np.argmax(q[4*i + j])] for j in range(4)])\n",
    "        \n",
    "def argmax_tiebreak(array):\n",
    "    return np.random.choice(np.where(array == array.max())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904ec408",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discrete_Q_Agent:\n",
    "    def __init__(self, action_space, observation_space, epsilon=0.9):\n",
    "        self.action_space = action_space\n",
    "        self.observation_space = observation_space\n",
    "        self.epsilon = epsilon\n",
    "        self.reset_Q()\n",
    "    \n",
    "    def reset_Q(self):\n",
    "        all_states = list(range(self.observation_space.n))\n",
    "        self.actions = list(range(self.action_space.n))\n",
    "        self.Q = {s: np.zeros(self.action_space.n) for s in all_states}\n",
    "\n",
    "    def action(self, state):\n",
    "# your code goes here\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b55655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sarsa(env, q_agent, alpha=0.1, gamma=0.99, rollouts=10000):\n",
    "    # your code goes here\n",
    "    return q_agent, q_agent.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfadf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_Learning(env, q_agent, alpha=0.1, gamma=0.99, rollouts=10000):\n",
    "    # your code goes here\n",
    "    return q_agent, q_agent.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7993db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_slippery = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "q_agent = Discrete_Q_Agent(env_slippery.action_space, env_slippery.observation_space, epsilon=0.9)\n",
    "q_agent, q = Sarsa(env_slippery, q_agent)\n",
    "visualize_q_fct(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91535fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_slippery = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "q_agent = Discrete_Q_Agent(env_slippery.action_space, env_slippery.observation_space, epsilon=0.9)\n",
    "q_agent, q = Q_Learning(env_slippery, q_agent)\n",
    "visualize_q_fct(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4eab0e",
   "metadata": {},
   "source": [
    "### 2. b) Cartpole\n",
    "Next, try the [Cartpole](https://www.gymlibrary.ml/environments/classic_control/cart_pole/) environment. It has a continuous state space, so we need to adjust our methods to accomodate that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f036dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac79b2d8",
   "metadata": {},
   "source": [
    "### 2. c) Cartpole learning\n",
    "The observation space of the Cartpole environment can be accessed with `env.observation_space`. It is a [`Box`](https://gymnasium.farama.org/api/spaces/fundamental/#box) space, which contains lower bounds, upper bounds, number of dimensions, and datatype. The second and forth dimension are unbounded. We can make them bounded by clipping every value over a certain threshold. Also, the first and third dimension have higher admissbable bounds, than is useful during training!\n",
    "\n",
    "Hint: Binned Q-Learning is not the most efficient or useful algorithm for this problem. With the provided hyperparameters I achieved only a mean reward of ~100 after 50000 rollouts of training without any further tuning. Can you achieve a better result by changing the hyperparameters or employing some additional technique?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b052a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "discounting_rate = 0.95\n",
    "number_episodes = 50000\n",
    "total_reward = 0\n",
    "\n",
    "q_table = np.zeros([31, 31, 51, 51, 2])\n",
    "window_size = np.array([0.25, 0.25, 0.01, 0.1])\n",
    "low_clip = [-3.75, -3.75, -0.25, -2.5]\n",
    "high_clip = [3.75, 3.75, 0.25, 2.5]\n",
    "\n",
    "# your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73ee792",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "bagent = Binned_Q_Agent_Cartpole(window_size, q_table)\n",
    "binned_q_learning(env, bagent, num_episodes=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93474134",
   "metadata": {},
   "source": [
    "### 3.a) Linear function control\n",
    "Implement the linear gradient Sarsa here. Most of the time after a few thousend episodes the linear policy is able to solve the problem (500 reward), but sometimes it just does not converge. The algorithm is a bit shakey as is! I also needed to add one little tweak: Normalize the state by clipping it, just as in the task before, and then dividing by the clip-value. This normalizes the state-vectors to [-1,1] and stablizes the algorithm.\n",
    "\n",
    "Note that for a linear formulation of Q_theta(., a), Grad(Q_theta(., a)) at state vector s is just that state vector s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70c03df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Q_Agent:\n",
    "    def __init__(self, action_space, observation_space, epsilon=0.9):\n",
    "        self.action_space = action_space\n",
    "        self.observation_space = observation_space\n",
    "        self.epsilon = epsilon\n",
    "        self.theta = np.zeros((action_space.n, observation_space.shape[0]))\n",
    "        \n",
    "    def norm_state(self, state):\n",
    "        norm_state = state\n",
    "        norm_state = np.clip(norm_state,low_clip,high_clip)\n",
    "        norm_state /= high_clip\n",
    "        return norm_state\n",
    "\n",
    "# your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330b4e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_agent = Linear_Q_Agent(env.action_space, env.observation_space)\n",
    "lin_agent = Grad_Sarsa(env, lin_agent, rollouts=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9638e6",
   "metadata": {},
   "source": [
    "### 3.b) DQN\n",
    "As a suggestion, I provided the interfaces for functions, some hyperparameters, and the architecture of the neural net that approximates Q. For this algorithm to somewhat work, I needed at least experience replay. But other techniques may also be interesting and work even better. Please feel free to experiment!\n",
    "\n",
    "*Note*: 1. Whenever you either `model.predict` oder `model.fit` you can gain a lot of performance if you do it as a batch. E.g. use \n",
    "```\n",
    "X = []\n",
    "y = []\n",
    "for i in I:\n",
    "    X.append(get_data(i))\n",
    "    y.append(get_label(i))\n",
    "model.fit(X,y)\n",
    "```\n",
    "instead of\n",
    "```\n",
    "for i in I:\n",
    "    model.fit(get_data(i), get_label(i))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de617670",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_size = 2000\n",
    "epsilon = 0.05\n",
    "learning_rate = 0.001\n",
    "\n",
    "class DQN_Agent:\n",
    "    def _init_model(self, state_dim, action_dim, learning_rate):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(32, input_dim=state_dim, activation='relu'))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dense(action_dim, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=learning_rate))\n",
    "        return model\n",
    "        \n",
    "    def action(self, state):\n",
    "        pass\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        pass\n",
    "\n",
    "    def learn_from_replay(self, batch_size):\n",
    "        pass\n",
    "    \n",
    "def DQN(env, agent, replay_batch_size=128, rollouts=2000):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f78b5b",
   "metadata": {},
   "source": [
    "### 3.c) Another one\n",
    "Browse the [environments](https://gymnasium.farama.org/) to pick another challenge! Maybe even record a video with the [RecordVideo wrapper](https://gymnasium.farama.org/api/wrappers/misc_wrappers/#gymnasium.wrappers.RecordVideo)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c377104",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
